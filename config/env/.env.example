# Synopticon API - Environment Configuration Template
# Copy this file to .env and customize for your deployment

# =============================================================================
# Core Configuration
# =============================================================================

# Runtime Environment
NODE_ENV=production
HOST=0.0.0.0
PORT=3000

# =============================================================================
# LLM Configuration (Required for full functionality)
# =============================================================================

# LLM Service URL - External Ollama instance
LLM_API_URL=http://host.docker.internal:11434
# Alternative for Docker Compose: http://ollama:11434
# Alternative for localhost: http://localhost:11434

# LLM Backend Selection
LLM_BACKEND=ollama
# Options: ollama, mock (for development/testing)

# LLM Model Configuration
LLM_MODEL=llama3.2
# Other options: llama3.1, codellama, mistral

# LLM Generation Parameters
TEMPERATURE=0.7
MAX_TOKENS=150

# =============================================================================
# Network Configuration
# =============================================================================

# CORS Origins (comma-separated)
CORS_ORIGINS=http://localhost:3000,http://127.0.0.1:3000,http://localhost:8080

# Speech Analysis Server URL
SPEECH_SERVER_URL=http://localhost:3000/api/analyze

# Transport Layer Configuration
TRANSPORT_BASE_URL=http://localhost:8080
WEBSOCKET_HOST=localhost

# =============================================================================
# Eye Tracking Configuration
# =============================================================================

# Neon Eye Tracker Device Address
NEON_DEVICE_ADDRESS=localhost
# Set to actual device IP for hardware integration

# Eye Tracking Port
NEON_DEVICE_PORT=8080

# =============================================================================
# Session Management
# =============================================================================

# Maximum concurrent sessions
MAX_SESSIONS=100

# Session timeout in milliseconds (1 hour = 3600000)
SESSION_TIMEOUT=3600000

# Session cleanup interval in milliseconds (5 minutes = 300000)
CLEANUP_INTERVAL=300000

# =============================================================================
# Analytics and Features
# =============================================================================

# Enable conversation analytics
ENABLE_ANALYTICS=true

# Enable audio quality monitoring
ENABLE_AUDIO_QUALITY=true

# Enable conversation flow analysis
ENABLE_CONVERSATION_FLOW=true

# =============================================================================
# Performance Configuration
# =============================================================================

# Memory pool sizes for optimization
MEMORY_POOL_SIZE=100

# Batch processing configuration
BATCH_SIZE=10
BATCH_TIMEOUT=2000

# =============================================================================
# Development & Debug Settings
# =============================================================================

# Debug logging (uncomment for development)
# DEBUG=synopticon:*

# Mock services for development
# USE_MOCK_DEVICES=true
# USE_MOCK_LLM=true

# =============================================================================
# Security Configuration
# =============================================================================

# Rate limiting (requests per 15 minutes)
RATE_LIMIT_MAX=1000

# Maximum request payload size
MAX_PAYLOAD_SIZE=10mb

# =============================================================================
# Docker-Specific Settings
# =============================================================================

# Container health check interval (seconds)
HEALTHCHECK_INTERVAL=30

# Container startup grace period (seconds)
HEALTHCHECK_START_PERIOD=40

# =============================================================================
# Cloud Platform Settings (Optional)
# =============================================================================

# For cloud deployments, override networking as needed
# CLOUD_PLATFORM=aws|gcp|azure
# EXTERNAL_LLM_ENDPOINT=https://your-llm-service.com/api

# =============================================================================
# Research & Academic Settings
# =============================================================================

# Data export formats
# EXPORT_FORMATS=json,csv,txt,md

# Research mode settings
# RESEARCH_MODE=true
# DATA_RETENTION_DAYS=30